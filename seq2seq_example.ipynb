{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from seq2seq.rnn_seq2seq import create_seq2seq_model, create_seq2seq_experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! rm -r ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_num = 1\n",
    "\n",
    "cell = \"LSTMCell\"\n",
    "num_layers = 2\n",
    "num_units = 16\n",
    "bidirectional = False\n",
    "attention = False\n",
    "residual_connections = True\n",
    "residual_dense = True\n",
    "\n",
    "vocab_size = 10\n",
    "emb_size = 5\n",
    "\n",
    "training_mode = \"scheduled_sampling_embedding\"\n",
    "scheduled_sampling_probability = 0.5\n",
    "\n",
    "inference_mode = \"beam\"\n",
    "beam_width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "        raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    "        yield [\n",
    "            np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.contrib.learn.python.learn.learn_io.generator_io import generator_input_fn\n",
    "from seq2seq.input.generator_io import generator_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator_py():\n",
    "    def generator():\n",
    "        data_gen = random_sequences(9, 9, 2, 9, 1)\n",
    "        for _ in range(1000):\n",
    "            data = next(data_gen)\n",
    "            \n",
    "            data = np.array(data[0], dtype=np.int32)\n",
    "            data_len = len(data)\n",
    "            data_len = np.array(data_len, dtype=np.int32)\n",
    "\n",
    "            yield {\n",
    "                \"inputs\": data,\n",
    "                \"inputs_length\": data_len,\n",
    "                \"targets\": data,\n",
    "                \"targets_length\": data_len\n",
    "            }\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg_train_input_fn = generator_input_fn(\n",
    "    x=data_generator_py(), \n",
    "    target_key=[\"targets\", \"targets_length\"], \n",
    "    batch_size=16, shuffle=False, num_epochs=None, \n",
    "    queue_capacity=128, num_threads=2, \n",
    "    pad_value=0)\n",
    "dg_test_input_fn = generator_input_fn(\n",
    "    x=data_generator_py(), \n",
    "    target_key=[\"targets\", \"targets_length\"], \n",
    "    batch_size=16, shuffle=False, num_epochs=None, \n",
    "    queue_capacity=128, num_threads=1, \n",
    "    pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_fn = create_seq2seq_experiment_fn(\n",
    "    dg_train_input_fn, dg_test_input_fn, \n",
    "    train_steps=int(2e4), eval_steps=int(1e2), min_eval_frequency=int(1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_option = 0.5  # Yeap, Estimator can use memory limitations\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_option)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    session_config=tf.ConfigProto(gpu_options=gpu_options),\n",
    "    model_dir=\"./logs\")\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "    cell_num=cell_num,\n",
    "    vocab_size=vocab_size, embedding_size=emb_size,\n",
    "    cell=cell, num_layers=num_layers, num_units=num_units,\n",
    "    bidirectional=bidirectional, attention=attention,\n",
    "    residual_connections=residual_connections, \n",
    "    residual_dense=residual_dense,\n",
    "    training_mode=training_mode,\n",
    "    learning_rate=1e-4,\n",
    "    lr_decay_steps=100000,\n",
    "    lr_decay_koef=0.99,\n",
    "    gradient_clip=10.0,\n",
    "    scheduled_sampling_probability=scheduled_sampling_probability,\n",
    "    inference_mode=inference_mode,\n",
    "    beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scitator/anaconda3/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 20001, 'loss': 1.4225006}, [])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR) # cause we don't want to log anything\n",
    "tf.contrib.learn.learn_runner.run(\n",
    "    experiment_fn=experiment_fn,\n",
    "    run_config=run_config,\n",
    "    schedule=\"train_and_evaluate\",\n",
    "    hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg_pred_input_fn = generator_input_fn(\n",
    "    x=data_generator_py(), \n",
    "    target_key=[\"targets\", \"targets_length\"], \n",
    "    batch_size=1, shuffle=False, num_epochs=None, \n",
    "    queue_capacity=128, num_threads=1, \n",
    "    pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_seq2seq_model(config=run_config, hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(dg_pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': array([[4, 7, 7],\n",
       "        [7, 4, 4],\n",
       "        [7, 7, 3],\n",
       "        [3, 3, 7],\n",
       "        [3, 3, 3],\n",
       "        [3, 3, 3],\n",
       "        [7, 7, 3],\n",
       "        [3, 3, 7],\n",
       "        [2, 2, 8],\n",
       "        [1, 1, 1]], dtype=int32),\n",
       " 'score': array([[ -1.30532646,  -1.40234065,  -2.01590633],\n",
       "        [ -2.61503649,  -2.67113137,  -2.75085378],\n",
       "        [ -3.86323595,  -3.9266541 ,  -3.98291349],\n",
       "        [ -4.84883213,  -4.90147924,  -5.0093708 ],\n",
       "        [ -5.78528881,  -5.83146334,  -5.87631416],\n",
       "        [ -6.80954361,  -6.83886814,  -6.84264088],\n",
       "        [ -8.07672405,  -8.10203648,  -8.12807655],\n",
       "        [ -9.42596436,  -9.43244457,  -9.45780849],\n",
       "        [-10.97326565, -10.99789333, -10.99912453],\n",
       "        [-11.07780743, -11.07973576, -11.10590267]], dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
